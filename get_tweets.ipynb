{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import constants\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keys & Create Tweepy Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bearer_token = constants.TWITTER_BEARER_TOKEN\n",
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(#billsmafia OR #gobills OR #bills) -is:retweet lang:en -#dirtybirds -#falcons -#birdcityfootball -#cardinals -#ravensflock -#ravens -#keeppounding -#panthers -#dabears -#bearsnation -#bears -#rulethejungle -#bengals -#whodey -#browns -#clevelandbrowns  -#dawgpound -#dallascowboys -#cowboys -#broncoscountry -#letsride -#broncos -#onepride -#lions  -#gopackgo -#packers -#wearetexans -#texans -#fortheshoe -#colts -#duuuval -#jags -#jaguars -#chiefskingdom -#chiefs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from team_queries import team_queries\n",
    "team_queries[str('bills')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Team Search - No Pagination\n",
    "Can use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Tweet id=1599840401468235776 text='We have 12 days until kickoff. This forecast will change 200 times until then. And I expect to see someone post every single one ðŸ˜‚ðŸ˜‚ it is great #BillsMafia https://t.co/9mbI5l6SXA'>,\n",
       " <Tweet id=1599840307754827776 text='Just a reminder for #billsmafia not winning MVP doesnt make Josh Allen any less elite.'>,\n",
       " <Tweet id=1599840253128617984 text='I unironically want there to be a severe blizzard just before kickoff of this game for... reasons... #BillsMafia https://t.co/fCNB0qqJnr'>,\n",
       " <Tweet id=1599840162988507144 text='@MadGlab Kyle Williams, forever and always. #GoBills https://t.co/VuioCnw0pO'>,\n",
       " <Tweet id=1599839901146415106 text='@IsaiahHodgins Congrats!  Good luck is stretch run... from #billsmafia'>,\n",
       " <Tweet id=1599839589597732864 text='Here we goâ€¦â€¦@MiamiDolphins you might want to bundle upâ€¦.it might be chilly ðŸ¥¶ â„ï¸. Actually weâ€™re hoping for a blizzardâ€¦..you cooked us in the heat so only right we freeze your ðŸ¥œ off. Bills by a Billion #Billsmafia #FinsUp'>,\n",
       " <Tweet id=1599839535092928516 text='Where Will Odell Beckham Jr. Sign? #BuffaloBills #Buffalo #GoBills #BillsNation https://t.co/PVH0PBCeNa'>,\n",
       " <Tweet id=1599839426942799876 text='I wonder how the players feel about having a game moved to Saturday. #GoBills'>,\n",
       " <Tweet id=1599839390305427457 text='#BillsMafia or whatever https://t.co/GsiqhyTWtG'>,\n",
       " <Tweet id=1599839389827231745 text='LETS GO #BILLSMAFIA\\u202f\\u202f\\u202f ðŸ˜¤ðŸ’ª! https://t.co/3HHpfaYCT7'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build search tweets \n",
    "# will use paginate normally, test case\n",
    "user_fields = [\"id\"]\n",
    "expansions = [\"author_id\"]\n",
    "\n",
    "# max_results min = 10\n",
    "def build_team_search(team_query, max_results = 10):\n",
    "    team_response = client.search_recent_tweets(\n",
    "            team_query, \n",
    "            max_results=max_results, \n",
    "            tweet_fields=[\"author_id\", \"created_at\", \"text\"], \n",
    "            user_fields=user_fields, \n",
    "            expansions=expansions\n",
    "        )\n",
    "    return team_response\n",
    "\n",
    "bills_response = build_team_search(team_queries[str('bills')], max_results=10)\n",
    "bills_response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get times and games from schedule data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get games from schedule\n",
    "2. For each game\n",
    "    a. get both teams\n",
    "    b. get date & time\n",
    "    c. get week\n",
    "    d. get home and away ? not sure need this\n",
    "3. For each game create database of tweets for each team\n",
    "    a. build query\n",
    "        i. Queries are always the same, can just create a dict of queries\n",
    "    b. get start time (4 days before game?) and end time (1 hr before game)\n",
    "    b. pagination function\n",
    "        ii. start time & end time\n",
    "    b. create database of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Schedule CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_schedule_csv(filepath):\n",
    "    sched_df = pd.read_csv(filepath, infer_datetime_format=True)\n",
    "\n",
    "    # convert all datetime fields to datetime type\n",
    "    sched_df['GameTime'] = pd.to_datetime(sched_df['GameTime'])\n",
    "    sched_df['SearchStartTime'] = pd.to_datetime(sched_df['SearchStartTime'])\n",
    "    sched_df['SearchEndTime'] = pd.to_datetime(sched_df['SearchEndTime'])\n",
    "\n",
    "    # convert datetime to twitter appropriate format for datetime requests\n",
    "    sched_df['GameTime'] = sched_df['GameTime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    sched_df['SearchStartTime'] = sched_df['SearchStartTime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    sched_df['SearchEndTime'] = sched_df['SearchEndTime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    return sched_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>SearchStartTime</th>\n",
       "      <th>SearchEndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>8:15 PM</td>\n",
       "      <td>buccaneers</td>\n",
       "      <td>saints</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2022-12-06T01:15:00Z</td>\n",
       "      <td>2022-12-02T00:15:00Z</td>\n",
       "      <td>2022-12-06T00:15:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Week  Day        Date     Time        home    away  home_score  \\\n",
       "14    13  Mon  2022-12-05  8:15 PM  buccaneers  saints          17   \n",
       "\n",
       "    away_score              GameTime       SearchStartTime  \\\n",
       "14          16  2022-12-06T01:15:00Z  2022-12-02T00:15:00Z   \n",
       "\n",
       "           SearchEndTime  \n",
       "14  2022-12-06T00:15:00Z  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_13 = './schedules/schedule_wk_13_cleaned.csv'\n",
    "\n",
    "sched_13_df = import_schedule_csv(filepath_13)\n",
    "sched_13_df_mon = sched_13_df[-1:]\n",
    "sched_13_df_mon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination\n",
    "\n",
    "Requests limited to 100 tweets at a time\n",
    "\n",
    "Must paginate requests using ```tweepy.Paginator```   \n",
    "```limit```: sets how many pages  \n",
    "```max_results```: sets how many results per page (limited to 100)  \n",
    "Total Results = limit * max_results  \n",
    "\n",
    "```start_time```: start of when to retrieve tweets  \n",
    "```end_time```: end of when to retrieve tweets  \n",
    "**Note**: times are in format of ```2022-11-24T15:25:00Z```  \n",
    "This is ZULU time which is **7 hours ahead of MST**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Team Paginator Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check start time within 7 days\n",
    "\n",
    "def check_start_time_not_greater_7_days(start_time):\n",
    "    week_ago = dt.datetime.utcnow() - pd.to_timedelta('7 days')\n",
    "    # print('1 week ago:', week_ago, '\\t Start_time: ', start_time)\n",
    "    check = week_ago < dt.datetime.strptime(start_time, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    # if (check == False):\n",
    "    #     print('Start time too old')\n",
    "    # else:\n",
    "    #     print('Start time allowed')\n",
    "    return check\n",
    "\n",
    "check_start_time_not_greater_7_days('2022-11-23T17:55:00Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_time=2019-01-01T17:00:00Z\n",
    "# end_time=2020-12-12T01:00:00Z\n",
    "#2022-11-23T20:05:00Z\t\n",
    "\n",
    "def paginate_search(team_query, max_results, start_time, end_time, limit=20):\n",
    "    start_valid = check_start_time_not_greater_7_days(start_time)\n",
    "    tweet_fields = [\"author_id\", \"created_at\", \"text\"]\n",
    "    \n",
    "    if (start_valid == True):\n",
    "        paginator = tweepy.Paginator(\n",
    "            client.search_recent_tweets, \n",
    "            query = team_query, \n",
    "            tweet_fields = tweet_fields, \n",
    "            start_time=start_time, \n",
    "            end_time=end_time,\n",
    "            max_results = max_results,\n",
    "            limit = limit\n",
    "        )\n",
    "        return paginator\n",
    "    else:\n",
    "        # run query with start time being one week ago + 1 minute\n",
    "        # otherwise time becomes invalid by time last pages come through, need the buffer\n",
    "        start_time = dt.datetime.utcnow() - pd.to_timedelta('7 days') + pd.to_timedelta('1 minute')\n",
    "        print('Start Time Not Valid', team_query[2:30], '\\n New Sart Time: ', start_time)\n",
    "        paginator = tweepy.Paginator(\n",
    "            client.search_recent_tweets, \n",
    "            query = team_query, \n",
    "            tweet_fields = tweet_fields, \n",
    "            start_time=start_time, \n",
    "            end_time=end_time,\n",
    "            max_results = max_results,\n",
    "            limit = limit\n",
    "        )\n",
    "        return paginator\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Paginator Search from Game Schedule\n",
    "Iterate through each game row  \n",
    "Get all data needed  \n",
    "Call paginae search for both home and away teams  \n",
    "Add to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paginator_dict_key(wk, team):\n",
    "    team_key = 'wk_' + str(wk) + '_' + team\n",
    "    return team_key\n",
    "\n",
    "def create_week_paginations_from_schedule(wk_sched_df, team_queries, max_results = 100, limit = 50):\n",
    "    # create empty paginators dict\n",
    "    paginators = {}\n",
    "\n",
    "    # iterate through each game row\n",
    "    for game_row in wk_sched_df.itertuples():\n",
    "        wk = game_row.Week\n",
    "\n",
    "        # get search start and end times\n",
    "        \n",
    "        start_time = game_row.SearchStartTime\n",
    "        end_time = game_row.SearchEndTime\n",
    "        \n",
    "        #get home and away teams\n",
    "        home_team = game_row.home\n",
    "        away_team = game_row.away\n",
    "\n",
    "        # get team queries\n",
    "        home_team_query = team_queries[str(home_team)]\n",
    "        away_team_query = team_queries[str(away_team)]\n",
    "\n",
    "        # create team paginators for home and away\n",
    "        home_team_paginator = paginate_search(\n",
    "            home_team_query, \n",
    "            max_results, \n",
    "            start_time, \n",
    "            end_time,\n",
    "            limit\n",
    "        )\n",
    "        away_team_paginator = paginate_search(\n",
    "            away_team_query, \n",
    "            max_results, \n",
    "            start_time, \n",
    "            end_time,\n",
    "            limit\n",
    "        )\n",
    "\n",
    "        # create dict key from         \n",
    "        home_key = get_paginator_dict_key(wk, home_team)\n",
    "        away_key = get_paginator_dict_key(wk, away_team)\n",
    "\n",
    "        # add paginators to dict\n",
    "        paginators[home_key] = {\n",
    "            \"paginator\": home_team_paginator,\n",
    "            \"team\": home_team\n",
    "        }\n",
    "        paginators[away_key] = {\n",
    "            \"paginator\": away_team_paginator,\n",
    "            \"team\": away_team\n",
    "        }\n",
    "        \n",
    "    return paginators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV file for team\n",
    "- Tweet Team\n",
    "- Tweet Text\n",
    "- Tweet Author\n",
    "- Tweet Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tweepy\n",
    "\n",
    "def create_tweet_save_path (week, key):\n",
    "    return  f'./team_data/wk_{week}/{key}_tweets.csv'\n",
    "\n",
    "\n",
    "# 'team_data/%s_tweets.csv' % (key)\n",
    "def create_dataset(paginator_dict, key, week_num):\\\n",
    "    # get team and paginator\n",
    "    team = paginator_dict[str(\"team\")]\n",
    "    paginator = paginator_dict[str(\"paginator\")]\n",
    "\n",
    "    # create save path for tweet csv's\n",
    "    file_save_path = create_tweet_save_path(week_num, key)\n",
    "    \n",
    "    with open(file_save_path, 'w', encoding=\"utf-8\") as file:\n",
    "        w = csv.writer(file)\n",
    "        \n",
    "        # Write header row (feature column names of your choice)\n",
    "        w.writerow(['team',\n",
    "                     'timestamp', \n",
    "                     'tweet_text', \n",
    "                     'userid' \n",
    "                     ])\n",
    "        # initiate tweet and page counts\n",
    "        tweet_count = 0\n",
    "        page_count = 0\n",
    "        \n",
    "        # iterate through pages of tweets\n",
    "        for page in paginator: \n",
    "            page_count += 1\n",
    "\n",
    "            # if no data found record and skip\n",
    "            if (page.data == None):\n",
    "                print(f'NO DATA FOR {team}')\n",
    "                print(page.meta)\n",
    "            \n",
    "            else:\n",
    "                # For each tweet add to csv file\n",
    "                for tweet in page.data:\n",
    "                    tweet_count += 1\n",
    "                \n",
    "                    w.writerow([team,\n",
    "                                tweet.created_at, \n",
    "                                tweet.text.replace('\\n',' ').encode('utf-8'), \n",
    "                                tweet.author_id, \n",
    "                                ])\n",
    "        # record data\n",
    "        print(f'Team: {team} \\t\\t Page Count: {page_count} \\t Tweet Count: {tweet_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_team_datasets(team_paginators, week_num):\n",
    "    # iterate through team paginator dict\n",
    "    for key, paginator_dict in team_paginators.items():\n",
    "        # print('KEY:%s - PAGINATOR: %s ' %(key, paginator_dict))\n",
    "        # pass each to create_dataset\n",
    "        create_dataset(paginator_dict, key, week_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Code to get all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>SearchStartTime</th>\n",
       "      <th>SearchEndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>Mon</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>8:15 PM</td>\n",
       "      <td>buccaneers</td>\n",
       "      <td>saints</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2022-12-06T01:15:00Z</td>\n",
       "      <td>2022-12-02T00:15:00Z</td>\n",
       "      <td>2022-12-06T00:15:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Week  Day        Date     Time        home    away  home_score  \\\n",
       "14    13  Mon  2022-12-05  8:15 PM  buccaneers  saints          17   \n",
       "\n",
       "    away_score              GameTime       SearchStartTime  \\\n",
       "14          16  2022-12-06T01:15:00Z  2022-12-02T00:15:00Z   \n",
       "\n",
       "           SearchEndTime  \n",
       "14  2022-12-06T00:15:00Z  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sched_13_df_thurs = sched_13_df.iloc[:1]\n",
    "sched_13_df_no_monday = sched_13_df.iloc[8:]\n",
    "sched_13_df_no_monday\n",
    "sched_13_df_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: buccaneers \t\t Page Count: 20 \t Tweet Count: 1984\n",
      "Team: saints \t\t Page Count: 18 \t Tweet Count: 1726\n"
     ]
    }
   ],
   "source": [
    "week_num = '13'\n",
    "paginators_wk13 = create_week_paginations_from_schedule(sched_13_df_mon, team_queries, 100)\n",
    "create_all_team_datasets(paginators_wk13, week_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to filter out multiteam hashtags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fc6efb3340bf4aae142c4471c3414bb5b17e6e80ba42a259676c40f0503db89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
