{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = constants.TWITTER_PUBLIC_API_KEY\n",
    "consumer_secret = constants.TWITTER_PRIVATE_API_KEY\n",
    "access_token = constants.TWITTER_ACCESS_TOKEN\n",
    "access_token_secret = constants.TWITTER_PRIVATE_ACCESS_TOKEN\n",
    "bearer_token = constants.TWITTER_BEARER_TOKEN\n",
    "\n",
    "# auth = tweepy.OAuth1UserHandler(\n",
    "#    consumer_key, consumer_secret, access_token, access_token_secret\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tweepy Client using bearer token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = tweepy.Client(bearer_token=bearer_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build query\n",
    "\n",
    "Want to have it exclude if multiple team tags  \n",
    "Can't include them all however as would go over the query character limit of ~512 \n",
    "\n",
    "Would limit the amount of tweets coming in however would also have to then filter again afterwards to remove the hastags that I couldan't include in the query  \n",
    "\n",
    "Approach 1:\n",
    "- build query and include first 50% of negated hashtags \n",
    "- filter tweets after using 2nd half of hashtags\n",
    "\n",
    "Approach 2:\n",
    "- build query and leave all hashtags\n",
    "- filter it afterwards for tweets with any other hashtag\n",
    "\n",
    "No idea which is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_team_hashtags_to_negate(team_hashtags, hashtag_list):\n",
    "    output = [b for b in hashtag_list if all(a not in b for a in team_hashtags)]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Length:  466\n",
      "Query Length:  466\n",
      "Query Length:  471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(#DaBears OR #BearsNation OR #Bears) -is:retweet lang:en -#DirtyBirds -#Falcons -#BirdCityFootball -#Cardinals -#RavensFlock -#Ravens -#BillsMafia -#GoBills -#Bills -#KeepPounding -#panthers -#RuleTheJungle -#Bengals -#WhoDey -#Browns -#ClevelandBrowns  -#DawgPound -#DallasCowboys -#Cowboys -#BroncosCountry -#LetsRide -#Broncos -#OnePride -#Lions  -#GoPackGo -#Packers -#WeAreTexans -#Texans -#ForTheShoe -#Colts -#DUUUVAL -#Jags -#Jaguars -#ChiefsKingdom -#Chiefs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashtags_sm\n",
    "from hashtags_stripped import ht_flat, ht_half_1, ht_half_2\n",
    "\n",
    "# test team list\n",
    "team_list = [\"bears\", \"bengals\", \"saints\"]\n",
    "\n",
    "# build twitter query for a team \n",
    "# take team name in dictionary and hashtag list\n",
    "# Ex \"(#RuleTheJungle OR #Bengals OR #WhoDey) -is:retweet\"\n",
    "# Important to have the parathensis around the main hashtags or else the retweet parameter won't work\n",
    "\n",
    "def build_team_query(team_name, hashtag_doc, hashtags_all):\n",
    "    \n",
    "    # retrieve team hashtags in list\n",
    "    team_hashtags = hashtag_doc[str(team_name)]\n",
    "\n",
    "    #retrieve non-team hashtags in list\n",
    "    negate_team_hashtags = get_non_team_hashtags_to_negate(team_hashtags, hashtags_all)\n",
    "    \n",
    "    # build start of query\n",
    "    query = '(' + team_hashtags[0]\n",
    "    \n",
    "    # if more than one hashtag, combine using OR statements\n",
    "    if (len(team_hashtags) > 1): \n",
    "        for tag in team_hashtags[1:]:\n",
    "            query = query + ' OR ' + tag\n",
    "\n",
    "    # filter out retweets? - not sure if I should do this or not, likely retweet something you also think\n",
    "    query = query + \") -is:retweet lang:en\"\n",
    "    \n",
    "    # filter out other team hashtags\n",
    "    for tag in negate_team_hashtags:\n",
    "        # stop if length of query is above 500\n",
    "        # limit is 512\n",
    "        if len(query) > 495:\n",
    "            break\n",
    "        query = query + \" -\"+ tag\n",
    "    print(\"Query Length: \", len(query))\n",
    "    return query\n",
    "\n",
    "def get_all_team_queries(team_list, team_hashtags, negate_hashtags):\n",
    "    queries = {}\n",
    "    for team in team_list:\n",
    "        queries[team] = build_team_query(team ,team_hashtags, negate_hashtags)\n",
    "    return queries\n",
    "\n",
    "\n",
    "team_queries = get_all_team_queries(team_list, hashtags_sm.hashtags, ht_half_1)\n",
    "team_queries[str('bears')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build search tweets \n",
    "\n",
    "user_fields = [\"id\"]\n",
    "expansions = [\"author_id\"]\n",
    "\n",
    "# max_results min = 10\n",
    "def build_team_search(team_query, max_results = 10):\n",
    "    team_response = client.search_recent_tweets(team_query, max_results=max_results, tweet_fields=tweet_fields, user_fields=user_fields, expansions=expansions)\n",
    "    return team_response\n",
    "\n",
    "# bengals_response = build_team_search(query_bengals, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination\n",
    "\n",
    "Requests limited to 100 tweets at a time\n",
    "\n",
    "Must paginate requests using ```tweepy.Paginator```   \n",
    "```limit```: sets how many pages  \n",
    "```max_results```: sets how many results per page (limited to 100)  \n",
    "Total Results = limit * max_results  \n",
    "\n",
    "```start_time```: start of when to retrieve tweets  \n",
    "```end_time```: end of when to retrieve tweets  \n",
    "**Note**: times are in format of ```2022-11-24T15:25:00Z```  \n",
    "This is ZULU time which is **7 hours ahead of MST**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def paginate_search(team_query, max_results, limit=20):\n",
    "    tweet_fields = [\"author_id\", \"created_at\", \"text\"]\n",
    "    paginator = tweepy.Paginator(\n",
    "        client.search_recent_tweets, \n",
    "        query = team_query, \n",
    "        tweet_fields = tweet_fields, \n",
    "        # start_time='2022-11-24T18:16:15Z', \n",
    "        # end_time='2022-11-24T22:16:15Z',\n",
    "        max_results = max_results,\n",
    "        limit = limit\n",
    "    )\n",
    "    # .flatten(limit = limit)\n",
    "\n",
    "    return paginator\n",
    "\n",
    "    # start_time=2019-01-01T17:00:00Z\n",
    "    # end_time=2020-12-12T01:00:00Z\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paginate for Each Team Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_team_paginators(team_queries, max_results=10, limit = 3):\n",
    "    # create empty dict for resulting paginators\n",
    "    paginators = {}\n",
    "\n",
    "    # iterate through dict\n",
    "    for team_key, team_query in team_queries.items():\n",
    "        team_paginator = paginate_search(team_query, max_results, limit)\n",
    "        paginators[team_key] = team_paginator\n",
    "    return paginators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_paginators = get_all_team_paginators(team_queries, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create team paginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bears': <tweepy.pagination.Paginator at 0x1f273363790>,\n",
       " 'bengals': <tweepy.pagination.Paginator at 0x1f274c268f0>,\n",
       " 'saints': <tweepy.pagination.Paginator at 0x1f274c26cb0>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_paginators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put tweets in dataframe or some format\n",
    "\n",
    "- Tweet Team\n",
    "- Tweet Text\n",
    "- Tweet Author\n",
    "- Tweet Date Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CSV file for team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import tweepy\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "\n",
    "def create_dataset(paginator, team):\n",
    "    with open('team_data/%s_data.csv' % (team), 'w', encoding=\"utf-8\") as file:\n",
    "        w = csv.writer(file)\n",
    "        \n",
    "        # Write header row (feature column names of your choice)\n",
    "        w.writerow(['team',\n",
    "                     'timestamp', \n",
    "                     'tweet_text', \n",
    "                     'userid' \n",
    "                     ])\n",
    "        for page in paginator: \n",
    "            # For each tweet matching hashtag, write relevant info to the spreadsheet\n",
    "            for tweet in page.data:\n",
    "                w.writerow([team,\n",
    "                            tweet.created_at, \n",
    "                            tweet.text.replace('\\n',' ').encode('utf-8'), \n",
    "                            tweet.author_id, \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_team_datasets(team_paginators):\n",
    "    # iterate through team paginator dict\n",
    "    for team_key, team_paginator in team_paginators.items():\n",
    "        # pass each to create_dataset\n",
    "        create_dataset(team_paginator, team_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_team_datasets(team_paginators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to filter out multiteam hashtags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fc6efb3340bf4aae142c4471c3414bb5b17e6e80ba42a259676c40f0503db89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
